{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Scrapy Beginners Series Part 1: How To Build Your First Production Scraper](https://scrapeops.io/python-scrapy-playbook/scrapy-beginners-guide/)\n",
    "\n",
    "# Scrapy 初心者向けシリーズ パート 1: 初めてのプロダクション スクレーパーを構築する方法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python Scrapy 5部構成の初心者向けシリーズ\n",
    "- Part 1: Basic Scrapy Spider\n",
    "  - Scrapyの基本を確認し、最初のScrapyスパイダーを構築します。\n",
    "- Part 2：汚れたデータのクリーニングとエッジケースの処理\n",
    "  - Webデータは厄介で構造化されておらず、多くのエッジケースを持つことができます。\n",
    "  - Items、Itemloaders、Item Pipelineを使用して、これらのエッジケースに対してスパイダーを堅牢にします。\n",
    "- Part 3：データの保存\n",
    "  - データベース、CSVファイル、JSONフォーマット、S3バケットからスクレイピングしたデータを保存する方法はさまざまです。\n",
    "  - データを保存するさまざまな方法を検討し、その長所、短所、どのような場面で使うかについて説明します。\n",
    "- Part4：ユーザーエージェントとプロキシ\n",
    "  - ユーザーエージェントとIPを管理し、ブロックされないようにすることで、スパイダーを本番環境に対応させます。\n",
    "- Part 5: デプロイメント、スケジューリング、ジョブの実行\n",
    "  - サーバー上にスパイダーをデプロイし、[ScrapeOps](https://scrapeops.io/)を介してジョブの監視とスケジューリングを行う。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CSSセレクタとXPath表現の解析\n",
    "- データ形式（CSV、JSON、XML）およびストレージ（FTP、S3、ローカルファイルシステム）\n",
    "- 堅牢なエンコーディングサポート\n",
    "- 同時実行管理\n",
    "- 自動再試行\n",
    "- クッキーとセッションの処理\n",
    "- クロールスパイダーと内蔵のページネーションサポート"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginners Scrapy Tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Setup your Python Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Scrapy Is Installed\n",
    "\n",
    "```bash\n",
    "(venv) c01 (😁 :main *) :$ scrapy\n",
    "\n",
    "Scrapy 2.9.0 - no active project\n",
    "\n",
    "Usage:\n",
    "  scrapy <command> [options] [args]\n",
    "\n",
    "Available commands:\n",
    "  bench         Run quick benchmark test\n",
    "  fetch         Fetch a URL using the Scrapy downloader\n",
    "  genspider     Generate new spider using pre-defined templates\n",
    "  runspider     Run a self-contained spider (without creating a project)\n",
    "  settings      Get settings values\n",
    "  shell         Interactive scraping console\n",
    "  startproject  Create new project\n",
    "  version       Print Scrapy version\n",
    "  view          Open URL in browser, as seen by Scrapy\n",
    "\n",
    "  [ more ]      More commands available when run from project directory\n",
    "\n",
    "Use \"scrapy <command> -h\" to see more info about a command\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Setup Our Scrapy Project\n",
    "\n",
    "```bash\n",
    "# scrapyプロジェクトの作成\n",
    "(venv) c01 (😁 :main *) :$ scrapy startproject chocolatescraper\n",
    "\n",
    "New Scrapy project 'chocolatescraper', using template directory '/Users/takeru/@LEARNING/Python/python-scrapy-playbook-scrapyops/venv/lib/python3.10/site-packages/scrapy/templates/project', created in:\n",
    "    /Users/takeru/@LEARNING/Python/python-scrapy-playbook-scrapyops/c01/chocolatescraper\n",
    "\n",
    "You can start your first spider with:\n",
    "    cd chocolatescraper\n",
    "    scrapy genspider example example.com\n",
    "```\n",
    "\n",
    "```bash\n",
    "# chocolatescraperディレクトリに移動\n",
    "cd chocolatescraper\n",
    "```\n",
    "\n",
    "```bash\n",
    "# chocolatescraperディレクトリのtree\n",
    "(venv) chocolatescraper (😁 :main *) :$ tree\n",
    "\n",
    ".\n",
    "├── chocolatescraper\n",
    "│   ├── __init__.py\n",
    "│   ├── items.py\n",
    "│   ├── middlewares.py\n",
    "│   ├── pipelines.py\n",
    "│   ├── settings.py\n",
    "│   └── spiders\n",
    "│       └── __init__.py\n",
    "└── scrapy.cfg\n",
    "\n",
    "3 directories, 7 files\n",
    "```\n",
    "\n",
    "- `settings.py`\n",
    "  - パイプラインやミドルウェアの有効化など、プロジェクトのすべての設定が含まれる場所です。\n",
    "  - 遅延、同時実行、その他多くのことを変更することができます。\n",
    "- `items.py`\n",
    "  - 抽出されたデータのモデルです。\n",
    "  - Scrapy Itemクラスを継承し、スクレイピングされたデータを含むカスタムモデル（ProductItemのような）を定義することができます\n",
    "- `pipelines.py`\n",
    "  - スパイダーが生成したアイテムが渡される場所で、主にテキストのクリーニングやファイル出力やデータベース（CSV、JSON SQLなど）への接続に使用します。\n",
    "- `middlewares.py`\n",
    "  - リクエストが行われ、scrapy がレスポンスを処理する方法を変更したいときに便利です。\n",
    "- `scrapy.cfg`\n",
    "  - いくつかのデプロイ設定などを変更するための設定ファイルです。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3- Creating Our Spider\n",
    "\n",
    "- Spider\n",
    "  - `start_url` のリストを受け取り、それぞれを `parse` メソッドでスクレイピングする。\n",
    "- CrawlSpider \n",
    "  - 見つけたリンクをたどって、ウェブサイト全体をクロールするように設計されている。\n",
    "- SitemapSpider\n",
    "  - サイトマップから URL を抽出するように設計されてる。\n",
    "\n",
    "```bash\n",
    "# 新しい汎用スパイダーを作成するには、`genspider`コマンドを実行するだけです：\n",
    "# scrapy genspider <name_of_spider> <website>\n",
    "(venv) chocolatescraper (😁 :main *) :$ scrapy genspider chocolatespider chocolate.co.uk\n",
    "\n",
    "Created spider 'chocolatespider' using template 'basic' in module:\n",
    "  chocolatescraper.spiders.chocolatespider\n",
    "```\n",
    "\n",
    "これで `spiders` フォルダに新しいスパイダーが追加され、次のようになります：\n",
    "\n",
    "```python\n",
    "# chocolatespider.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class ChocolatespiderSpider(scrapy.Spider):\n",
    "    name = 'chocolatespider'\n",
    "    allowed_domains = ['chocolate.co.uk']\n",
    "    start_urls = ['http://chocolate.co.uk/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "```\n",
    "\n",
    "ここでは、`genspider`コマンドが、`Spider`クラスという形で、私たちが使用するためのテンプレートスパイダーを作成したことを確認します。\n",
    "\n",
    "このスパイダー・クラスには以下のものがあります：\n",
    "\n",
    "- name\n",
    "  - スパイダーに名前をつけるクラス属性です。\n",
    "  - あとで `scrapy crawl <spider_name>` としてスパイダーを実行するときに、この名前を使用します。\n",
    "- allowed_domains\n",
    "  - クラス属性で、Scrapyに `chocolate.co.uk` ドメインのページのみをスクレイピングするように指示します。\n",
    "  - スパイダーが暴走して多くのウェブサイトをスクレイピングするのを防ぐためです。\n",
    "  - これはオプションです。\n",
    "- start_urls\n",
    "  - Scrapyにスクレイピングすべき最初のURLを指示するクラス属性です。\n",
    "  - これは少しづつ変更していきます。\n",
    "- parse\n",
    "  - ターゲットウェブサイトからレスポンスを受信した後、`parse`関数が呼び出されます。\n",
    "\n",
    "このSpiderを使い始めるには、2つのことをする必要があります：\n",
    "\n",
    "- start_urls`をスクレイピングしたいURL <https://www.chocolate.co.uk/collections/all>に変更する。\n",
    "- パースコードを `parse` 関数に挿入する。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 - Update Start Urls\n",
    "\n",
    "```python\n",
    "# chocolatespider.py\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class ChocolatespiderSpider(scrapy.Spider):\n",
    "    name = 'chocolatespider'\n",
    "    allowed_domains = ['chocolate.co.uk']\n",
    "    start_urls = ['https://chocolate.co.uk/collections/all']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n",
    "```\n",
    "\n",
    "次に、ページから必要なデータを解析するためのCSSセレクタを作成する必要があります。\n",
    "\n",
    "これを行うには、Scrapy Shellを使用します。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 - Scrapy Shell: Finding Our CSS Selectors\n",
    "\n",
    "HTMLページからデータを抽出するには、[XPath](https://www.w3schools.com/xml/xpath_intro.asp)または[CSSセレクタ](https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Selectors)を使用して、Scrapyにページのどこにデータがあるのかを伝える必要があります。XPathとCSSセレクタは、ScrapyがDOMツリーをナビゲートし、必要なデータの場所を見つけるための小さなマップのようなものです。このガイドでは、ページからデータを解析するためにCSSセレクタを使用するつもりです。そして、これらのCSSセレクタを作成するために、私たちは[Scrapy Shell](https://docs.scrapy.org/en/latest/topics/shell.html)を使用する予定です。\n",
    "\n",
    "Scrapyの素晴らしい機能の1つは、あなたがすぐにあなたのXPathとCSSセレクタをテストし、デバッグすることができ、組み込みのシェルが付属していることです。XPathやCSSのセレクタが正しいかどうかを確認するためにスクレイパーを実行する代わりに、ターミナルに直接入力して結果を確認することができます。\n",
    "\n",
    "```bash\n",
    "# Scrapyシェルを開くには、次のコマンドを使用します：\n",
    "(venv) chocolatescraper (😁 :main *) :$ scrapy shell\n",
    "\n",
    "2023-05-29 20:06:24 [scrapy.utils.log] INFO: Scrapy 2.9.0 started (bot: chocolatescraper)\n",
    "2023-05-29 20:06:24 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.11 (main, Apr 24 2023, 17:34:58) [Clang 14.0.3 (clang-1403.0.22.14.1)], pyOpenSSL 23.1.1 (OpenSSL 3.1.0 14 Mar 2023), cryptography 40.0.2, Platform macOS-13.3.1-x86_64-i386-64bit\n",
    "2023-05-29 20:06:24 [scrapy.crawler] INFO: Overridden settings:\n",
    "{'BOT_NAME': 'chocolatescraper',\n",
    " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
    " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
    " 'LOGSTATS_INTERVAL': 0,\n",
    " 'NEWSPIDER_MODULE': 'chocolatescraper.spiders',\n",
    " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
    " 'ROBOTSTXT_OBEY': True,\n",
    " 'SPIDER_MODULES': ['chocolatescraper.spiders'],\n",
    " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
    "2023-05-29 20:06:24 [asyncio] DEBUG: Using selector: KqueueSelector\n",
    "2023-05-29 20:06:24 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
    "2023-05-29 20:06:24 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
    "2023-05-29 20:06:24 [scrapy.extensions.telnet] INFO: Telnet Password: f3038c0cc138d466\n",
    "2023-05-29 20:06:24 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    " 'scrapy.extensions.telnet.TelnetConsole',\n",
    " 'scrapy.extensions.memusage.MemoryUsage']\n",
    "2023-05-29 20:06:24 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2023-05-29 20:06:24 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2023-05-29 20:06:24 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2023-05-29 20:06:24 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
    "2023-05-29 20:06:24 [asyncio] DEBUG: Using selector: KqueueSelector\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x10dcf76a0>\n",
    "[s]   item       {}\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x10dcf6ce0>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser\n",
    "2023-05-29 20:06:25 [asyncio] DEBUG: Using selector: KqueueSelector\n",
    "In [1]: \n",
    "```\n",
    "\n",
    "```bash\n",
    "# scrapy shellを抜ける\n",
    "In [1]: quit\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch The Page\n",
    "\n",
    "CSSセレクタを作成するために、次のページでテストします：\n",
    "\n",
    "<https://www.chocolate.co.uk/collections/all>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
